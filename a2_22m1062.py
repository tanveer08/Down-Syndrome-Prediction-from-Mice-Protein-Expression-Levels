# -*- coding: utf-8 -*-
"""A2_22M1062.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fO_zW2-1CL8VsD5tzB8ejY5DxxGU-hdo

## Assignment 2

### Name : Tanveer Sharma
### Roll No. : 22M1062

Link for Video Explanation : https://iitbacin.sharepoint.com/:v:/s/Assignment5745/EWORoKwklfpPqMprfFpUkz8BgNEKn-Z25ciLFXj75anRkQ?e=OfuoEe

### **Objective 1**

Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from scipy.stats import ttest_ind
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

"""#### Task 1
- In this task we have to read the dataset from direct link 
- here initial 77 columns are  input variables from which we will extract the features.
- last two columns represent the classifier data
"""

url = "https://www.ee.iitb.ac.in/~asethi/Dump/MouseTrain.csv"
datafile = pd.read_csv(url)

print(datafile)

"""#### Task 2 Exploratory Data Analysis"""

# Get an overview of the data
print(datafile.info())

"""the above code  gives us some basic information regarding the data
- firstly about number of samples in each column
- secondly about the datatype of each column.From here it can be inferred that upto first 77 columns we have float values after that for last two columns we have string data i.e. Classes 
"""

datafile.describe()

"""The above describe method provides us with the mean value and standard deviation of the each variable. It also gives us range of data in it i.e. the minimun and maximum values."""

datafile.isna()#Checking for missing values
#datafile.fillna(datafile.mean())#Fill every empty value by the mean of each value

"""the above code is for checking the the missing samples in the variables.
 - We can also use datafile.fillna(datafile.mean())to fill every empty value by the mean of each column(variable)
 - But since we will be using imputation in the further steps we are not using this.

Now we have generated histograms of each variables and heat map of correlation between variables. 
- This is just for the better understanding of data and finding whether we can remove some data varaibles (not relevent or highly correlated) or not.
"""

print(datafile.dtypes)

# Generate histograms of all numerical variables in the DataFrame
datafile.hist(figsize=(15,10))
plt.xlabel('Data')
plt.ylabel('Frequency')
plt.title('Histogram of Data')
plt.show()

# Generate a heatmap of the correlation between variables
sns.heatmap(datafile.corr())
plt.xlabel('Data')
plt.ylabel('Density')
plt.title('Density Plot of Data')
plt.show()

"""Now I have dropped some of the variables with samples less than 700"""

counts = datafile.count()
# Print the counts for each column
print(len(counts))
less_than_700 = counts[counts < 700]
# Print the names of the columns that have less than 700 non-missing values
print(less_than_700.index)
print(less_than_700)
# Drop the columns that have less than 700 non-missing values
datafile = datafile.drop(less_than_700.index, axis=1)
datafile.count()

"""Therefore we have dropped 5 input variables here.

Now we have used the label encoder to convert our output classes into integer form 
- label_encoder_1 is used for binary classification
- label_encoder_2 is used for quatenary classification
- this code also helps us understand the number of classes i.e. it presents data classification in more clearer way.
"""

X = datafile.iloc[:, :-2].values
# Example output classes represented as strings
# Initialize the LabelEncoder object
label_encoder_1= LabelEncoder()
label_encoder_2= LabelEncoder()
# Fit the label encoder to the output classes
label_encoder_1.fit(datafile['Genotype'])
label_encoder_2.fit(datafile['Treatment_Behavior'])
# Transform the output classes to numerical values
y1 = label_encoder_1.transform(datafile['Genotype'])
y2 =  label_encoder_2.transform(datafile['Treatment_Behavior']) 
print(X.shape)
print(y1)
print(y2)

"""Now we have drop the one of the two varaibles which have very high correlation.
- for that first we will introduce t- test and try to understand it.
- secondly we will find effect of two variables on binary classification as output values

#### T- Test on Binary Classification Data
"""

# Split the data into two classes based on the target variable
class_1 = X[y1 == 0]
class_2 = X[y1 == 1]

# Perform a t-test for each feature
for i in range(X.shape[1]):
    feature = X[:, i]
    t_statistic, p_value = ttest_ind(class_1[:, i], class_2[:, i])
    print(f"Feature {i}: t-value={t_statistic:.3f}, p-value={p_value:.3f}")

"""The t-value represents the difference between the means of two populations normalized by the standard deviation of the sample. It is calculated as:

t-value = (mean1 - mean2) / (s / sqrt(n))

where mean1 and mean2 are the means of the two populations, s is the pooled standard deviation of the two samples, and n is the number of observations in each sample.

The t-value represents how much the means of the two populations differ in standard deviation units. If the t-value is large, it means that the means are significantly different. Conversely, if the t-value is small, it means that the means are similar.
In feature selection, a higher p-value generally indicates that the null hypothesis, which is that there is no significant difference between the means of the two populations for a particular feature, cannot be rejected. In other words, the feature may not be informative or useful in distinguishing between the two classes.

Therefore, in general, we would want to select features with lower p-values, indicating that there is a significant difference between the means of the two populations for those features. These features may be more informative and useful in distinguishing between the two classes.
"""

corr_matrix = datafile.corr()
target_col = y1#datafile['Genotype']
dropped_columns = []  # create an empty list to store dropped column indices
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > 0.95:
            col1 = datafile.iloc[:,[i]]
            col2 = datafile.iloc[:,[j]]
            print(f"highly correlated column pair : ({i},{j})")
            t1_stat, p1_val = ttest_ind(col1, target_col)
            t2_stat, p2_val = ttest_ind(col2, target_col, equal_var=True)
            
            if abs(t1_stat) > abs(t2_stat):
                datafile.drop(col2, axis=1,inplace = True)
                dropped_columns.append(j)  # store the index of the dropped column
                print(f"Dropped column {i}")
            else:
                datafile.drop(col1, axis=1,inplace = True)
                dropped_columns.append(i)  # store the index of the dropped column
                print(f"Dropped column {j}")
                break
print(datafile.shape)
print("Indices of dropped columns:", dropped_columns)

"""Now to extract features from variables we will import Standard Scaler library and use fit transform method on X to obtain train data."""

X = datafile.iloc[:, :-2].values
sc = StandardScaler()
X_train = sc.fit_transform(X)
print(X_train.shape)

"""#### Test Data Pre Processing

Now to extract features from test data I have followed the following steps.
- Firstly i have read test data directly from url given in the assignment.
- Next i have dropped columns in the test data file that are not present in the train data file.
- Lastly i have extract features from the test variables by using the fit method on SC which has been transformed using train data.
"""

url = "https://www.ee.iitb.ac.in/~asethi/Dump/MouseTest.csv"
datafile_test = pd.read_csv(url)
datafile_test.isna()#Checking for missing values
#datafile_test.fillna(datafile.mean())#Fill every empty value by the mean of each value
#print(datafile_test)
#datafile_test = datafile_test.drop(less_than_700.index, axis=1)# dropping the variables which has less than 700 samples in train data
cols1 = datafile.columns.tolist()
X_test = datafile_test.iloc[:, :-2].values
# drop columns in the second data file that are not present in the first data file
cols2 = [col for col in datafile_test.columns.tolist() if col in datafile.columns.tolist()]
df2 = datafile_test[cols2]
X_test=df2.iloc[:,:-2].values
y1_test = datafile_test.iloc[:, -1].values
y2_test= datafile_test.iloc[:,-2].values
y1_test = label_encoder_1.transform(datafile_test['Genotype'])
y2_test = label_encoder_2.transform(datafile_test['Treatment_Behavior'])

print(X_test.shape)
X_test = sc.transform(X_test)

print(X_test)

"""Below Code is just for better understanding of Test data."""

datafile_test.describe()

"""####  Are the classes balanced ?

"""

print("For train data")
# count the frequency of unique values in a column
value_counts_binary = datafile['Genotype'].value_counts()
value_counts_quaternary = datafile['Treatment_Behavior'].value_counts()

print(value_counts_binary) # print the frequency of each class in binary
print(value_counts_quaternary) # print the frequency of each class in quaternary

print("For test data")
# count the frequency of unique values in a column
value_counts_binary = datafile_test['Genotype'].value_counts()
value_counts_quaternary = datafile_test['Treatment_Behavior'].value_counts()

print(value_counts_binary) # print the frequency of each class in binary
print(value_counts_quaternary) # print the frequency of each class in quaternary

"""From above results we can say that 
- for train data in both binary and quatenary classification classes are not balanced.
- for test data in  binary class is not  balanced.
- for test data in quatenary classification first 3 classes are equally present but 4th class is less in frequency.

#### Task 3
#### Iterative Imputation on Train Data and Test Data
"""

imp = IterativeImputer(max_iter=10, random_state=0)
imp.fit(X_train)
IterativeImputer(random_state=0)

 # the model learns that the second feature is double the first
X_train=imp.transform(X_train)
X_test=imp.transform(X_test)

"""Task 4

When evaluating the performance of classification models, there are several metrics that can be used depending on the nature of the classification problem. In this case, we have two separate classification tasks - one is binary, and the other has four classes. Below are some metrics that can be used to evaluate the performance of the classification models:

**Binary classification:**

**Accuracy:** The proportion of correct predictions made by the model.

**Precision:** The proportion of true positive predictions out of all positive predictions made by the model. Precision measures how accurate positive predictions are.

**Recall:** The proportion of true positive predictions out of all actual positive instances in the dataset. Recall measures how well the model identifies positive instances.

**F1 Score:** The harmonic mean of precision and recall. It gives an overall measure of the model's accuracy and ability to make correct predictions.

**ROC AUC :** (Receiver Operating Characteristic Area Under the Curve) is another commonly used metric for evaluating the performance of binary classification models. It measures the ability of a model to distinguish between positive and negative classes and can be useful in situations where the two classes are imbalanced or the cost of false positives and false negatives are different. A model with a higher ROC AUC value indicates better performance in distinguishing between positive and negative instances.

**Multiclass classification:**

 **Accuracy:** The proportion of correct predictions made by the model.

**Macro-averaged F1 Score:** The average of the F1 score for each class. This metric gives equal weight to each class, regardless of class imbalance.

**Micro-averaged F1 Score:** The F1 score calculated by considering all the true positives, false positives, and false negatives for all the classes. This metric gives equal weight to each observation, regardless of class imbalance.

**Weighted F1 Score:** The F1 score weighted by the number of observations in each class. This metric is useful when there is a class imbalance.

**Cohen's Kappa:** A measure of agreement between the predicted and actual labels, which takes into account chance agreement. It can be used for multiclass classification problems.

**ROC AUC score vs. F1 score in multiclass classification**

In a nutshell, the major difference between ROC AUC and F1 is related to class imbalance. Here is a summary of reading many StackOverflow threads on how to choose one over the other:

If we have a high class imbalance, always choose the F1 score because a high F1 score considers both precision and recall. To get a high F1, both false positives and false negatives must be low. On the other hand, ROC AUC can give precious high scores with a high enough number of false positives. Besides, we can also think of the ROC AUC score as the average of F1 scores (both good and bad) evaluated at various thresholds. Always use F1 when we have a class imbalance. Higher ROC AUC does not necessarily mean a better classifier.

###Task 5 a
"""

model1 = LinearSVC()
model1.fit(X_train, y1)
model2 = LinearSVC()
model2.fit(X_train, y2) 
# print prediction results
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
predictions2 = model2.predict(X_test)
print(classification_report(y2_test, predictions2))

# Define the parameter grid to search over
param_grid = {'C': [0.001,0.01,0.1, 1, 10, 100]}

# Create a linear SVM classifier
clf = LinearSVC(max_iter=1000000)

# Perform grid search with 5-fold cross-validation
grid_search1 = GridSearchCV(clf, param_grid, cv=5)
grid_search1.fit(X_train, y1)
grid_search2 = GridSearchCV(clf, param_grid, cv=5)
grid_search2.fit(X_train, y2)
# Print the best parameter and the corresponding mean cross-validation score
print("Best parameter for Binary Classification : ", grid_search1.best_params_)
print("Best score  for Binary Classification: ", grid_search1.best_score_)
print("Best parameter  for Quaternary Classification: ", grid_search2.best_params_)
print("Best score  for Quaternaery Classification: ", grid_search2.best_score_)

"""Part 5b"""

model1 = SVC(kernel='rbf')
model1.fit(X_train, y1)
model2 = SVC(kernel='rbf')
model2.fit(X_train, y2) 
# print prediction results
print("Test Data")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
predictions2 = model2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

# defining parameter range
param_grid = {'C': [0.1, 1, 10, 100, 1000,10000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001,0.00001],
              'kernel': ['rbf']} 
grid_1 = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,cv=5)
grid_1.fit(X_train,y1)
grid_2 = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,cv=5)
grid_2.fit(X_train,y2)
# fitting the model for grid search
print("Best parameter for Binary Classification : ", grid_1.best_params_)
print("Best score  for Binary Classification: ", grid_1.best_score_)
print("Best parameter  for Quaternary Classification: ", grid_2.best_params_)
print("Best score  for Quaternaery Classification: ", grid_2.best_score_)

model1 = SVC(C= 10, gamma= 0.001, kernel= 'rbf')
model1.fit(X_train, y1)
model2 = SVC(C= 1000, gamma= 0.0001, kernel= 'rbf')
model2.fit(X_train, y2) 
# print prediction results
print("Test Data")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
predictions2 = model2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

"""Part 5C

Neural network with single ReLU hidden layer and Softmax output

Network Weight Initialization
Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer.
Mostly uniform distribution is used.

Activation function
Activation functions are used to introduce nonlinearity to models, which allows deep learning models to learn nonlinear prediction boundaries.

Generally, the rectifier activation function is the most popular.

Sigmoid is used in the output layer while making binary predictions. Softmax is used in the output layer while making multi-class predictions.

The output layer is automatically set to use the Softmax activation function since MLPClassifier assumes multi-class classification problems.
"""

model1 = MLPClassifier(activation='relu', solver='adam', random_state=42)
model1.fit(X_train, y1)
model2 = MLPClassifier(activation='relu', solver='adam', random_state=42)
model2.fit(X_train, y2) 
# print prediction results
print("Test Data")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
predictions2 = model2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

neural_clf = MLPClassifier(activation='relu', solver='adam', random_state=42)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'hidden_layer_sizes': [(10,), (20,), (30,), (40,), (50,)],
    'alpha': [0.001, 0.01, 0.1, 1, 10]
}

# Define the grid search procedure
grid_1 = GridSearchCV(neural_clf, param_grid, refit = True, verbose = 3,cv=5)
grid_1.fit(X_train,y1)
grid_2 = GridSearchCV(neural_clf, param_grid, refit = True, verbose = 3,cv=5)
grid_2.fit(X_train,y2)
# Fit the grid search to the data
# fitting the model for grid search
print("Best parameter for Binary Classification : ", grid_1.best_params_)
print("Best score  for Binary Classification: ", grid_1.best_score_)
print("Best parameter  for Quaternary Classification: ", grid_2.best_params_)
print("Best score  for Quaternaery Classification: ", grid_2.best_score_)

"""Part 5 D"""

model1 = RandomForestClassifier()
model1.fit(X_train, y1)
model2 = RandomForestClassifier()
model2.fit(X_train, y2) 
# print prediction results
print("Test Data")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
predictions2 = model2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

param_grid = {
    'n_estimators': [25, 50, 100, 150],
    'max_depth': [3, 6, 9],
    'max_leaf_nodes': [3, 6, 9],
}
grid_1 = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 3,cv=5)
grid_1.fit(X_train,y1)
grid_2 = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 3,cv=5)
grid_2.fit(X_train,y2)
# fitting the model for grid search
print("Best parameter for Binary Classification : ", grid_1.best_params_)
print("Best score  for Binary Classification: ", grid_1.best_score_)
print("Best parameter  for Quaternary Classification: ", grid_2.best_params_)
print("Best score  for Quaternaery Classification: ", grid_2.best_score_)

"""#### Task 6

Feature importance is a technique used in machine learning to determine the relevance or importance of each feature (also known as variable or predictor) in predicting the target variable. There are different methods to calculate feature importance, and the choice of method depends on the type of model and the data.

Here are some common methods to calculate feature importance:

- Coefficient of the model: In linear regression models, the coefficients represent the importance of each feature. Features with larger absolute values of coefficients are more important than features with smaller coefficients.

- Feature importance from decision trees: Decision tree-based models, such as random forest and gradient boosting, can provide feature importance measures based on the number of times a feature is used to split the data and the reduction in impurity achieved by each split.

- Permutation feature importance: Permutation feature importance is a model-agnostic method that works for any type of model. It involves randomly permuting the values of each feature and measuring the drop in the model's performance (e.g., accuracy or AUC) when the feature is shuffled. Features that result in the largest drop in performance are considered more important.

- SHapley Additive exPlanations (SHAP): SHAP is a method to calculate feature importance in black-box models. It is based on the concept of Shapley values from cooperative game theory and provides a unified framework to explain the contribution of each feature to the prediction of the target variable.

To determine if the same proteins are important for each model, you can calculate the feature importance for each model using one or more of these methods and compare the results. If the same proteins consistently appear as important across different models, it suggests that these proteins are indeed important predictors of the target variable. However, if the importance rankings of features vary greatly between models, it may indicate that the models have learned different patterns in the data and that different features are important for each model.
"""

clf=SVC(kernel='linear')
print("For Binary Classification")
clf.fit(X_train,y1)
coef = clf.coef_
feature_importance = pd.DataFrame(coef.T, index=datafile.iloc[:, :-2].columns, columns=["importance"])
feature_importance = feature_importance.abs()
feature_importance = feature_importance.sort_values(by='importance', ascending = False)
print(feature_importance)

"""the above code doesn't work for rbf kernel and random forest classifier"""

model1 = SVC(kernel='rbf')
model1.fit(X_train, y1)
model2 = SVC(kernel='rbf')
model2.fit(X_train, y2) 
print("Binary Classification")
# print("Cofficients",model1.feature_importances_)
result = permutation_importance(model1, X_train, y1, n_repeats=10, random_state=42)
# Sort features by importance score in descending order
importances = result.importances_mean
indices = np.argsort(importances)[::-1]

# Print feature importance scores
for f in range(X_train.shape[1]):
    print("%d. %s (%f)" % (f + 1, datafile.iloc[:, :-2].columns[indices[f]], importances[indices[f]]))

print("Quatenary Classification")
# print("Cofficients",model1.feature_importances_)
result = permutation_importance(model2, X_train, y2, n_repeats=10, random_state=42)
# Sort features by importance score in descending order
importances = result.importances_mean
indices = np.argsort(importances)[::-1]

# Print feature importance scores
for f in range(X_train.shape[1]):
    print("%d. %s (%f)" % (f + 1, datafile.iloc[:, :-2].columns[indices[f]], importances[indices[f]]))

"""Feature Importance for Random Forest Classifier"""

clf=RandomForestClassifier()
print("For Binary Classification")
clf.fit(X_train,y1)
coef = clf.feature_importances_
#print("Cofficients",model1.feature_importances_)
feature_importance = pd.DataFrame(coef.T, index=datafile.iloc[:, :-2].columns, columns=["importance"])
feature_importance = feature_importance.abs()
feature_importance = feature_importance.sort_values(by='importance', ascending = False)
print(feature_importance)
print("For Quaternary Classification")
clf2=RandomForestClassifier()
clf2.fit(X_train,y2)
coef2 = clf2.feature_importances_
#print("Cofficients",model1.feature_importances_)
feature_importance = pd.DataFrame(coef2.T, index=datafile.iloc[:, :-2].columns, columns=["importance"])
feature_importance = feature_importance.abs()
feature_importance = feature_importance.sort_values(by='importance', ascending = False)
print(feature_importance)

result1 = permutation_importance(model1, X_train, y1, n_repeats=10, random_state=42)
for i in range(X_train.shape[1]):
    print(f"{i+1}. {result1.importances_mean[i]:.3f} +/- {result1.importances_std[i]:.3f}")
print("Quaternary Classification")
result2 = permutation_importance(model1, X_train, y2, n_repeats=10, random_state=42)
for i in range(X.shape[1]):
    print(f"{i+1}. {result2.importances_mean[i]:.3f} +/- {result2.importances_std[i]:.3f}")

"""We are not able to get anything out using permutation importance as most of the values are zero.

Feature Importance for Neural Network with Single Relu Hidden Layer
"""

print("Binary Classification")
clf_nn = MLPClassifier(activation='relu', solver='adam', random_state=42).fit(X_train, y1)
result = permutation_importance(clf_nn, X_train, y1, n_repeats=10, random_state=42)
importances = result.importances_mean
std = result.importances_std
feature_names = datafile.iloc[:, :-2].columns
indices = importances.argsort()[::-1][:10]

# Print the names and importances of the most important features
print("Top 10 Features:")
for i in indices:
    print(f"{feature_names[i]}: {importances[i]} +/- {std[i]}")

print(" Quatenary Classification")
clf_nn = MLPClassifier(activation='relu', solver='adam', random_state=42).fit(X_train, y2)
result = permutation_importance(clf_nn, X_train, y2, n_repeats=10, random_state=42)
importances = result.importances_mean
std = result.importances_std
feature_names = datafile.iloc[:, :-2].columns
indices = importances.argsort()[::-1][:10]

# Print the names and importances of the most important features
print("Top 10 Features:")
for i in indices:
    print(f"{feature_names[i]}: {importances[i]} +/- {std[i]}")

"""TAsk 7 Recursive Feature Elimination

(A) For Linear SVC
"""

min_features_to_select = 1  # Minimum number of features to consider
clf_linear = SVC(kernel="linear")
cv = StratifiedKFold(5)
print("Binary Classification")
rfecv_linearSVC_y1 = RFECV(estimator=clf_linear,step=1,cv=cv,scoring="accuracy",n_jobs=-1,)#here if scoring changed
rfecv_linearSVC_y1.fit(X_train, y1)
X_train_svm_lnear_recv_y1=rfecv_linearSVC_y1.fit_transform(X_train,y1)
print(f"Optimal number of features: {rfecv_linearSVC_y1.n_features_}")
print("New Feature Array",X_train_svm_lnear_recv_y1.shape)
n_scores = len(rfecv_linearSVC_y1.cv_results_["mean_test_score"])
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Mean test accuracy")
plt.errorbar(
    range(min_features_to_select, n_scores + min_features_to_select),
    rfecv_linearSVC_y1.cv_results_["mean_test_score"],
    yerr=rfecv_linearSVC_y1.cv_results_["std_test_score"],
)
plt.title("Recursive Feature Elimination \nwith correlated features")
plt.figure(figsize=(20, 8))
plt.show()
importances = abs(rfecv_linearSVC_y1.estimator_.coef_[0])
feature_names = datafile.iloc[:, :-2].columns

# Get the indices of the most important features
indices = importances.argsort()[::-1][:10]

# Print the names and importances of the most important features
print("Top 10 Features:")
for i in indices:
    print(f"{feature_names[i]}: {importances[i]}")

min_features_to_select = 1  # Minimum number of features to consider
clf_linear = SVC(kernel="linear")
cv = StratifiedKFold(5)
print("Quatenary Classification")
rfecv_linearSVC_y2 = RFECV(estimator=clf_linear,step=1,cv=cv,scoring="accuracy",n_jobs=-1,)#here if scoring changed
rfecv_linearSVC_y2.fit(X_train, y2)
X_train_svm_lnear_recv_y2=rfecv_linearSVC_y2.fit_transform(X_train,y2)
print(f"Optimal number of features: {rfecv_linearSVC_y2.n_features_}")
print("New Feature Array",X_train_svm_lnear_recv_y2.shape)
n_scores = len(rfecv_linearSVC_y2.cv_results_["mean_test_score"])
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Mean test accuracy")
plt.errorbar(
    range(min_features_to_select, n_scores + min_features_to_select),
    rfecv_linearSVC_y2.cv_results_["mean_test_score"],
    yerr=rfecv_linearSVC_y2.cv_results_["std_test_score"],
)
plt.title("Recursive Feature Elimination \nwith correlated features")
plt.figure(figsize=(20, 8))
plt.show()
importances = abs(rfecv_linearSVC_y2.estimator_.coef_[0])
feature_names = datafile.iloc[:, :-2].columns

# Get the indices of the most important features
indices = importances.argsort()[::-1][:10]

# Print the names and importances of the most important features
print("Top 10 Features:")
for i in indices:
    print(f"{feature_names[i]}: {importances[i]}")



"""Observation : Number of features changes by changing scoring to accuracy to f1

(B) For SVM with rbf Kernel

I tried this  below given code but wasn't able to resolve the Value Error: when `importance_getter=='auto'`, the underlying estimator SVC should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.
"""

# min_features_to_select = 1  # Minimum number of features to consider
# clf_rbf = SVC(kernel="rbf")
# cv = StratifiedKFold(5)

# # rfe_rbf = RFECV(estimator=svc_rbf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)
# # rfe_rf = RFECV(estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)
# rfecv_rbfSVC = RFECV(estimator=clf_rbf, step=1, cv=cv, scoring="accuracy", n_jobs=-1)
# rfecv_rbfSVC.fit(X_train,y1)

# X_train_svm_rbf_recv_y1 = rfecv_rbfSVC.transform(X_train)
# print(f"Optimal number of features: {rfecv_rbfSVC.n_features_}")
# print("New Feature Array", X_train_svm_rbf_recv_y1.shape)

# n_scores = len(rfecv_rbfSVC.cv_results_["mean_test_score"])
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Mean test accuracy")
# plt.errorbar(
#     range(min_features_to_select, n_scores + min_features_to_select),
#     rfecv_rbfSVC.cv_results_["mean_test_score"],
#     yerr=rfecv_rbfSVC.cv_results_["std_test_score"],
# )
# plt.title("Recursive Feature Elimination \nwith RBF kernel")
# plt.show()

"""(C) For Random Forest Classifier"""

min_features_to_select = 1  # Minimum number of features to consider
clf_rf = RandomForestClassifier()
cv = StratifiedKFold(5)
print("Binary Classification")
# rfe_rbf = RFECV(estimator=svc_rbf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)
# rfe_rf = RFECV(estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)
rfecv_rf_y1 = RFECV(estimator=clf_rf, step=1, cv=cv, scoring="accuracy", n_jobs=-1)
rfecv_rf_y1.fit(X_train,y1)

X_train_rf_recv_y1 = rfecv_rf_y1.transform(X_train)
print(f"Optimal number of features: {rfecv_rf_y1.n_features_}")
print("New Feature Array", X_train_rf_recv_y1.shape)

n_scores = len(rfecv_rf_y1.cv_results_["mean_test_score"])
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Mean test accuracy")
plt.errorbar(
    range(min_features_to_select, n_scores + min_features_to_select),
    rfecv_rf_y1.cv_results_["mean_test_score"],
    yerr=rfecv_rf_y1.cv_results_["std_test_score"],
)
plt.title("Recursive Feature Elimination \nwith Random Forest Classifier")
plt.show()
importances = rfecv_rf_y1.estimator_.feature_importances_
feature_names = datafile.iloc[:, :-2].columns

# Get the indices of the most important features
indices = importances.argsort()[::-1][:10]

# Print the names and importances of the most important features
print("Top 10 Features:")
for i in indices:
    print(f"{feature_names[i]}: {importances[i]}")

min_features_to_select = 1  # Minimum number of features to consider
clf_rf = RandomForestClassifier()
cv = StratifiedKFold(5)
print("Quatenary Classification")
# rfe_rbf = RFECV(estimator=svc_rbf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)
# rfe_rf = RFECV(estimator=rf, step=1, cv=cv, scoring='accuracy', n_jobs=-1)
rfecv_rf_y2 = RFECV(estimator=clf_rf, step=1, cv=cv, scoring="accuracy", n_jobs=-1)
rfecv_rf_y2.fit(X_train,y2)

X_train_rf_recv_y2 = rfecv_rf_y2.transform(X_train)
print(f"Optimal number of features: {rfecv_rf_y2.n_features_}")
print("New Feature Array", X_train_rf_recv_y2.shape)

n_scores = len(rfecv_rf_y2.cv_results_["mean_test_score"])
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Mean test accuracy")
plt.errorbar(
    range(min_features_to_select, n_scores + min_features_to_select),
    rfecv_rf_y2.cv_results_["mean_test_score"],
    yerr=rfecv_rf_y2.cv_results_["std_test_score"],
)
plt.title("Recursive Feature Elimination \nwith Random Forest Classifier")
plt.show()
importances = rfecv_rf_y2.estimator_.feature_importances_
feature_names = datafile.iloc[:, :-2].columns

# Get the indices of the most important features
indices = importances.argsort()[::-1][:10]

# Print the names and importances of the most important features
print("Top 10 Features:")
for i in indices:
    print(f"{feature_names[i]}: {importances[i]}")

"""(D) For Neural Network with single layer hidden layer

I tried this  below given code but wasn't able to resolve the Value Error: when `importance_getter=='auto'`, the underlying estimator SVC should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.
"""

# clf_nn = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', random_state=42)
# cv = StratifiedKFold(5)

# rfecv_nn = RFECV(estimator=clf_nn, step=1, cv=cv, scoring="accuracy", n_jobs=-1)
# rfecv_nn.fit(X_train, y1)

# X_train_rfecv_nn = rfecv_nn.transform(X_train)
# print(f"Optimal number of features: {rfecv_nn.n_features_}")
# print("New Feature Array", X_train_rfecv_nn.shape)

# n_scores = len(rfecv_nn.cv_results_["mean_test_score"])
# plt.figure()
# plt.xlabel("Number of features selected")
# plt.ylabel("Mean test accuracy")
# plt.errorbar(
#     range(1, n_scores + 1),
#     rfecv_nn.cv_results_["mean_test_score"],
#     yerr=rfecv_nn.cv_results_["std_test_score"],
# )
# plt.title("Recursive Feature Elimination with Neural Network")
# plt.show()

# # Get the coefficients of the trained neural network
# importances = rfecv_nn.estimator_.coef_[0]
# feature_names = datafile.iloc[:, :-2].columns

# # Get the indices of the most important features
# indices = importances.argsort()[::-1][:10]

# # Print the names and importances of the most important features
# print("Top 10 Features:")
# for i in indices:
#     print(f"{feature_names[i]}: {importances[i]}")

print(" Quatenary Classification")
clf_nn = MLPClassifier(activation='relu', solver='adam', random_state=42).fit(X_train, y2)
result = permutation_importance(clf_nn, X_train, y1, n_repeats=10, random_state=42)
importances = result.importances_mean
std = result.importances_std
feature_names = datafile.iloc[:, :-2].columns
indices = importances.argsort()[::-1][:10]

# Print the names and importances of the most important features
print("Top 10 Features:")
for i in indices:
    print(f"{feature_names[i]}: {importances[i]} +/- {std[i]}")

"""Part 8 Testing the Model

- Here while making the model we are setting the hyperparameters to be optimal values ehich were obtained earlier.
- We have compared model for binary and quaternary classification before and after the recursive feature elimination for the Random Forest and Linear SVC.
- We have compared model for binary and quaternary classification for the rbf kernel SVM and Neural network with single hidden layer with optimal hyperparameters.

In the below code we are comparing the output for Linear SVC for binary classification before and after removing the applying the Recursive Feature Elimination
"""

model1 = SVC(C= .01, kernel= 'linear')
model1.fit(X_train, y1)
model2 = SVC(C= .01, kernel= 'linear')
model2.fit(X_train_svm_lnear_recv_y1, y1) 
# print prediction results
print("Test Data for Binary Classification")
print("Data  with  features after data preprocessing")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
print("Data  with  features after Recursive Feature Elimination")
predictions2=rfecv_linearSVC_y1.predict(X_test)
print(classification_report(y1_test, predictions2))
print(confusion_matrix(y1_test, predictions2))

"""We can see the sharp increase in performance matrix after feature elimination"""

model1 = SVC(C= .1,kernel= 'linear')
model1.fit(X_train, y2)
model2 = SVC(C= .1, kernel= 'linear')
model2.fit(X_train_svm_lnear_recv_y1, y2) 
# print prediction results
print("Test Data for Quatenary Classification")
print("Data  with  features after data preprocessing")
predictions1 = model1.predict(X_test)
print(classification_report(y2_test, predictions1))
print(confusion_matrix(y2_test, predictions1))
print("Data  with  features after Recursive Feature Elimination")
predictions2=rfecv_linearSVC_y2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

"""In the below code we are comparing the output for SVC with rbf kernel for binary classification and Quatenary Classification."""

model1 = SVC(C= 10, gamma= 0.001, kernel= 'rbf')
model1.fit(X_train, y1)
model2 = SVC(C= 1000, gamma= 0.00001, kernel= 'rbf')
model2.fit(X_train, y2) 
# print prediction results
print("Test Data")
print("Binary Classification")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
print("Quaternary Classification")
predictions2=model2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

"""We can see that there is an increase in f1 score.

In the below code we are comparing the output for Random Forest Classifier for binary classification before and after removing the applying the Recursive Feature Elimination.
"""

model1 = RandomForestClassifier(max_depth= 9, max_leaf_nodes=9, n_estimators= 25)
model1.fit(X_train, y1)
model2 = RandomForestClassifier(max_depth= 9, max_leaf_nodes=9, n_estimators= 25)
model2.fit(X_train_rf_recv_y1, y1) 
# print prediction results
print("Test Data for Binary Classification")
print("Data  with  features after data preprocessing")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
print("Data  with  features after Recursive Feature Elimination")
predictions2=rfecv_linearSVC_y1.predict(X_test)
print(classification_report(y1_test, predictions2))
print(confusion_matrix(y1_test, predictions2))

"""We can see increase in both f1 score and accuracy."""

model1 = RandomForestClassifier(max_depth= 9, max_leaf_nodes=9, n_estimators= 25)
model1.fit(X_train, y2)
model2 = RandomForestClassifier(max_depth= 9, max_leaf_nodes=9, n_estimators= 25)
model2.fit(X_train_rf_recv_y2, y2) 
# print prediction results
print("Test Data for Quatenary Classification")
print("Data  with  features after data preprocessing")
predictions1 = model1.predict(X_test)
print(classification_report(y2_test, predictions1))
print(confusion_matrix(y2_test, predictions1))
print("Data  with  features after Recursive Feature Elimination")
predictions2=rfecv_rf_y2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

"""Here class with lower support has has decrease in performance metrics after recursive feature elimination.

In the below code we are comparing the output forNeural network with single ReLU hidden layer for binary classification and Quatenary Classification.
"""

model1 = MLPClassifier(hidden_layer_sizes=(30,),activation='relu', solver='adam',random_state=42,alpha= 1)
model1.fit(X_train, y1)
model2 = MLPClassifier(hidden_layer_sizes=(30,),activation='relu', solver='adam',random_state=42,alpha= 0.001)
model2.fit(X_train, y2) 
# print prediction results
print("Test Data")
print("Binary Classification")
predictions1 = model1.predict(X_test)
print(classification_report(y1_test, predictions1))
print(confusion_matrix(y1_test, predictions1))
print("Quaternary Classification")
predictions2=model2.predict(X_test)
print(classification_report(y2_test, predictions2))
print(confusion_matrix(y2_test, predictions2))

"""### **Objective  2** """

from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy
from torchvision.datasets import ImageFolder

cudnn.benchmark = True
plt.ion()

"""Here I have downloaded dataset directly from the given link and extract it using zipFile library. Earlier I was using Google Drive which would require to login to google drive(not recoomended)"""

# from google.colab import drive
# drive.mount('/content/drive')
import urllib.request
import zipfile
url='https://download.pytorch.org/tutorial/hymenoptera_data.zip'
filename = 'hymenoptera_data.zip'

# download the zip file from the URL
urllib.request.urlretrieve(url, filename)

# open the zip file
with zipfile.ZipFile(filename, 'r') as zip_ref:
    # extract all the files in the zip file to a folder
    zip_ref.extractall('data_folder')

# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

# data_dir = '/content/drive/MyDrive/hymenoptera_data/'
data_dir = 'data_folder/hymenoptera_data/'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),data_transforms[x])for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,shuffle=True, num_workers=4)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""Visualising few images."""

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))

# Make a grid from batch
out = torchvision.utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])

"""Training the model
Now, letâ€™s write a general function to train a model. Here, we will illustrate:

- Scheduling the learning rate

- Saving the best model
"""

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:4f}')

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

"""Visualizing the model predictions 

Generic function to display predictions for a few images
"""

def visualize_model(model, num_images=6):
    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloaders['val']):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(num_images//2, 2, images_so_far)
                ax.axis('off')
                ax.set_title(f'predicted: {class_names[preds[j]]}')
                imshow(inputs.cpu().data[j])

                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)

model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
model_ft.fc = torch.nn.Identity()
print(num_ftrs)
print(type(model_ft))

"""#### Task 10 Defining function to get image features"""

def get_resnet18_features(image_folder_path):
    # Define ResNet18 model
    resnet18 = models.resnet18(pretrained=True)
    resnet18.fc = torch.nn.Identity() # Remove final fully connected layer
    
    # Define data transformations
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Load image dataset
    image_dataset = ImageFolder(image_folder_path, transform=transform)
    resnet18_features = []
    labels = []
    for image,label in image_dataset:
            features = model_ft(image.unsqueeze(0))
            resnet18_features.append(features.detach().numpy().squeeze())
            labels.append(label)
    return (resnet18_features),np.array(labels)

image_datasets = ImageFolder('data_folder/hymenoptera_data/train')
train_features,labels_train = get_resnet18_features(image_datasets.root)

train_array = np.array(train_features)
print(labels_train.shape)
print(train_array.shape)
print(len(train_features))

image_datasets = ImageFolder('data_folder/hymenoptera_data/val')
test_features,labels_test = get_resnet18_features(image_datasets.root)
test_array = np.array(test_features)
print(labels_test.shape)
print((test_array))
print(test_array.shape)

"""#### Task 11
In this task we have to Compare L2 regularized logistic regression, RBF kernel SVM and random forest for the test data using accuracy and f1_score and matrix.
- I have created the three different classifier i.e.   L2 regularized logistic regression, RBF kernel SVM and random forest with GridSearch CV for finding best hyperparameters in the latter two.
- I have the classification report(for f1_score and accuracy) and confusion matrix for each classifier from which we can interpret the results
"""

# Load data
X_train = train_array
y_train = labels_train
X_test = test_array
y_test = labels_test


# L2 regularized logistic regression
lr = LogisticRegression(penalty='l2', max_iter=1000, random_state=42)
lr.fit(X_train, y_train)


# RBF kernel SVM
svm = SVC(kernel='rbf', random_state=42)
svm_params = {'C': [0.1, 1, 10,50,100], 'gamma': [0.01, 0.1, 1,10]}
svm_gs = GridSearchCV(svm, svm_params, scoring='accuracy', cv=5)
svm_gs.fit(X_train, y_train)


# Random forest
rf = RandomForestClassifier(random_state=42)
rf_params = {'n_estimators': [100, 200, 500,1000], 'max_depth': [0.5,1,2,5, 10, 20]}
rf_gs = GridSearchCV(rf, rf_params, scoring='accuracy', cv=5)
rf_gs.fit(X_train, y_train)


# Print results
print('Logistic regression:' )
print(classification_report(y_test, lr.predict(X_test)))
print(confusion_matrix(y_test, lr.predict(X_test)))
print('RBF kernel SVM:' )
print("Best parameter for SVM with Kernel RBF Classification : ", svm_gs.best_params_)
print("Best score  for SVM with Kernel RBF Classification: ", svm_gs.best_score_)
print(classification_report(y_test,svm_gs.predict(X_test)))
print(confusion_matrix(y_test,svm_gs.predict(X_test)))
print('Random forest: ')
print("Best parameter for Random Forest Classification : ", rf_gs.best_params_)
print("Best score  for Random Forest Classification: ", rf_gs.best_score_)
print(classification_report(y_test, rf_gs.predict(X_test)))
print(confusion_matrix(y_test, rf_gs.predict(X_test)))

"""### Findings : 
#### Objective 1 
- When we comapred before(find initially in task 5) and after finding the optimal parameters and we can say that we have find better accuracy and f1_score for both binary and quateary classification.
- Similiarly for after eliminating features using the RCECV we have found better accuracy and F1_Score for both binary and quateary classification

#### Comparison between different Models

**Binary Class**
- For binary class we have in terms of f1_score in quite less in Random Forest Classifier as compared to other models.
- Also the time taken in Random Forest to fit the data into model is high.
- Accuracy is highest in rbf kernel SVM as compared to other models for binary class.
- There in increase in both f1_score and accuracy after feature elimination using RCFEV.

**Quatenary Class**
- For f1_score  we have a least in Neural Network with Single Hidden Layer.
- For accuracy  also follow a similiar pattern
- Mostly time taken in quaternary is higher in all the models than  binary classification.

**Recursive Feature Elimination :**

It can be only done for Linear SVC and Random Forest Classifier
- In Random Forest optimal number of  for 4 features for binary classification and 25 features for quaternary numbers.
- In Linear SVC binary classification number of optimal features are 15 and for quaternary classifaction optimal number of features are 34.
- So we can say that random forest focuses on selecting less features and try to use them efficiently.

**Objective 2**

This objective focuses more on image datasets handling in machne learning. Major part of this objective is to extract  features from image dataset useing resnet 18 neural network.

- After finding the features we had compare 3 different models for classification purposes.
- The dataset provided easily implied that the problem is similiar to binary classification.
- the accuracy was minimum in RandomForest Model and slightly better in rbf kernel and linear SVM with latter leading by a value of .01
- thw F1_score follows the similiar pattern to accuracy.
- We can say that for image dataset generally we have low accuracy and f1_score of about 60%.

Code Discussed with friends : Only discussed most of the feature elimination and feature extratction in Objective i.e. Q2 Q6,Q7 in OBjective 1 and Part 10 in Objective 2.
1. Dinesh Kumar Panwar 22M1080
2. Karra Maneesha 22M1076

Resources for Objective 1 :
- https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd
- https://towardsdatascience.com/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362
- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
- https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py
- https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py
- https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/
- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/
- https://www.geeksforgeeks.org/random-forest-hyperparameter-tuning-in-python/?ref=rp


Resources for Objective 2: 
- https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html
"""